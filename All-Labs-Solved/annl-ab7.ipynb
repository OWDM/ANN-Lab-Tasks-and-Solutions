{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Implement the 2-layer network\n",
        "def logsiglayer(n):\n",
        "    return 1 / (1 + np.exp(-n))\n",
        "\n",
        "def linearlayer(n):\n",
        "    return n\n",
        "\n",
        "# Step 2: Implement the feedforward step\n",
        "def forwardpropagation(p, W1, b1, W2, b2):\n",
        "    a0 = p\n",
        "    a1 = logsiglayer(np.dot(W1, a0) + b1)\n",
        "    a2 = linearlayer(np.dot(W2, a1) + b2)\n",
        "    return a0, a1, a2\n",
        "\n",
        "# Step 3: Implement the derivatives of the transfer functions\n",
        "def dlogsig(n):\n",
        "    return (1 - logsiglayer(n)) * logsiglayer(n)\n",
        "\n",
        "def dpurelin(n):\n",
        "    return 1\n",
        "\n",
        "# Step 4: Implement the backpropagation step\n",
        "def backpropagation(F2, t, a, F1, W2):\n",
        "    s2 = -2 * F2 * (t - a)\n",
        "    s1 = F1 * np.dot(W2.T, s2)\n",
        "    return s2, s1\n",
        "\n",
        "# Step 5: Implement the update step\n",
        "def updateparams(W1, W2, b1, b2, s1, s2, a0, a1, a2, alpha):\n",
        "    W2_new = W2 - alpha * np.dot(s2, a1.T)\n",
        "    b2_new = b2 - alpha * s2\n",
        "    W1_new = W1 - alpha * np.dot(s1, a0.T)\n",
        "    b1_new = b1 - alpha * s1\n",
        "    return W1_new, W2_new, b1_new, b2_new\n",
        "\n",
        "# Step 6: Combine the steps in one function\n",
        "def backpropagatealgorithm(W1, W2, b1, b2, p, t, alpha):\n",
        "    a0, a1, a2 = forwardpropagation(p, W1, b1, W2, b2)\n",
        "    F2 = dpurelin(a2)\n",
        "    F1 = dlogsig(a1)\n",
        "    s2, s1 = backpropagation(F2, t, a2, F1, W2)\n",
        "    W1_new, W2_new, b1_new, b2_new = updateparams(W1, W2, b1, b2, s1, s2, a0, a1, a2, alpha)\n",
        "    return W1_new, W2_new, b1_new, b2_new\n",
        "\n",
        "# Question 7: Train the neural network using backpropagation\n",
        "# Randomly initialize the parameters\n",
        "np.random.seed(0)\n",
        "W1 = np.random.randn(4, 2)\n",
        "W2 = np.random.randn(1, 4)\n",
        "b1 = np.random.randn(4, 1)\n",
        "b2 = np.random.randn(1, 1)\n",
        "\n",
        "# Data points\n",
        "data = np.array([[-2, 0.0], [-1.5, -1], [0.29, -0.5], [0.617, 0.5], [1.38, 1.0], [1.707, 1.5], [1.92, 2.0]])\n",
        "\n",
        "# Training loop\n",
        "alpha = 0.1\n",
        "converged = False\n",
        "iterations = 0\n",
        "\n",
        "while not converged:\n",
        "    np.random.shuffle(data)\n",
        "    for point in data:\n",
        "        p = point[0]\n",
        "        t = point[1]\n",
        "        W1, W2, b1, b2 = backpropagatealgorithm(W1, W2, b1, b2, p, t, alpha)\n",
        "    iterations += 1\n",
        "    if iterations > 1000:  # Set a maximum number of iterations to avoid infinite loop\n",
        "        break\n",
        "\n",
        "# Converged values of Ws and bs\n",
        "print(\"Converged values:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"W2:\", W2)\n",
        "print(\"b1:\", b1)\n",
        "print(\"b2:\", b2)\n",
        "print(\"Number of iterations:\", iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C91h2dn9BzC",
        "outputId": "d482d3a1-7670-4652-f651-ea89e18c5fa6"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged values:\n",
            "W1: [[ 0.57810593  0.21453073]\n",
            " [ 0.68061657  1.45257479]\n",
            " [ 1.32743982 -0.31533309]\n",
            " [ 1.06443268  0.50376213]]\n",
            "W2: [[1.04822374 3.55854423 2.67998648 3.03498763]]\n",
            "b1: [[-2.52674232 -2.26303436]\n",
            " [-3.04524393 -2.91178474]\n",
            " [-3.28183102 -2.10399026]\n",
            " [-3.37251857 -2.11409212]]\n",
            "b2: [[-0.5405207  -0.98412145]]\n",
            "Number of iterations: 1001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q extra: How can you implement a neural network in a general way? In which you do not need to write\n",
        "another customized code for a 3-layer, 4-layer, or even more layers.\n",
        "\n",
        "To implement a neural network in a general way that can handle any number of layers, you can use a modular approach and create a class for the neural network. This allows you to define the number of layers and the number of neurons in each layer dynamically."
      ],
      "metadata": {
        "id": "mCsjNpI7UAL7"
      }
    }
  ]
}